# Prometheus Alert Rules for Mantis LGTM Stack
groups:
  - name: mantis_application_alerts
    interval: 30s
    rules:
      # High error rate alert
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_server_request_duration_seconds_count{http_response_status_code=~"5.."}[5m]))
            /
            sum(rate(http_server_request_duration_seconds_count[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # Slow response time alert
      - alert: SlowResponseTime
        expr: |
          histogram_quantile(0.95,
            rate(http_server_request_duration_seconds_bucket[5m])
          ) > 2
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "Slow response time detected"
          description: "P95 latency is {{ $value }}s (threshold: 2s)"

      # Low request rate (possible downtime)
      - alert: LowRequestRate
        expr: |
          sum(rate(http_server_request_duration_seconds_count[5m])) < 0.1
        for: 10m
        labels:
          severity: info
          component: application
        annotations:
          summary: "Low request rate detected"
          description: "Request rate is {{ $value }} req/s (threshold: 0.1 req/s)"

  - name: mantis_infrastructure_alerts
    interval: 30s
    rules:
      # OTel Collector is down
      - alert: OTelCollectorDown
        expr: up{job="otel-collector"} == 0
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "OpenTelemetry Collector is down"
          description: "OTel Collector has been down for more than 2 minutes"

      # Prometheus is down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus has been down for more than 2 minutes"

      # Loki is down
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Loki is down"
          description: "Loki has been down for more than 2 minutes"

      # Tempo is down
      - alert: TempoDown
        expr: up{job="tempo"} == 0
        for: 2m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Tempo is down"
          description: "Tempo has been down for more than 2 minutes"

      # Grafana is down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 2 minutes"

  - name: mantis_resource_alerts
    interval: 30s
    rules:
      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (process_resident_memory_bytes / process_virtual_memory_max_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
          component: resource
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      # Prometheus storage running low
      - alert: PrometheusStorageLow
        expr: |
          (prometheus_tsdb_storage_blocks_bytes / prometheus_tsdb_retention_limit_bytes) > 0.85
        for: 15m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "Prometheus storage is running low"
          description: "Prometheus storage is {{ $value | humanizePercentage }} full"
